以下参考源自http://www.cnblogs.com/skyme/archive/2011/10/28/2226850.html

**安装配置的过程写在了evernote**

##框架

Hadoop使用主/从（Master/Slave）架构，主要角色有NameNode，DataNode，secondary NameNode，JobTracker，TaskTracker组成。
其中NameNode，secondary NameNode，JobTracker运行在Master节点上，DataNode和TaskTracker运行在Slave节点上。  

1. NameNode。NameNode是HDFS的守护程序，负责记录文件是如何分割成数据块的，以及这些数据块被存储到哪些数据节点上。它的功能是对内存及I/O进行集中管理。
2. DataNode。集群中每个从服务器都运行一个DataNode后台程序，后台程序负责把HDFS数据块读写到本地文件系统。需要读写数据时，由NameNode告诉客户端去哪个DataNode进行具体的读写操作。
3. Secondary NameNode。Secondary NameNode是一个用来监控HDFS状态的辅助后台程序，提供周期检查点和清理任务。如果NameNode发生问题，可以使用Secondary NameNode作为备用的NameNode。
4. JobTracker。JobTracker后台程序用来连接应用程序与Hadoop，用户应用提交到集群后，由JobTracker决定哪个文件处理哪个task执行，一旦某个task失败，JobTracker会自动开启这个task。负责调度datanode上的工作。每个 datanode 有一个 tasktracker，它们执行实际工作。jobtracker 和 tasktracker 采用主-从形式，jobtracker 跨 datanode 分发工作，而 tasktracker 执行任务。jobtracker 还检查请求的工作，如果一个 datanode 由于某种原因失败，jobtracker 会重新调度以前的任务


##HDFS

  hadoop distributed file system   分布式文件系统
  

## 常用命令

hadoop dfs -ls 列出HDFS下的文件
hadoop dfs -ls in 列出HDFS下某个文档中的文件
hadoop dfs -put test1.txt test 上传文件到指定目录并且重新命名，只有所有的DataNode都接收完数据才算成功
hadoop dfs -get in getin 从HDFS获取文件并且重新命名为getin，同put一样可操作文件也可操作目录
hadoop dfs -rmr out 删除指定文件从HDFS上
hadoop dfs -cat in/* 查看HDFS上in目录的内容
hadoop dfsadmin -report 查看HDFS的基本统计信息，结果如下
hadoop dfsadmin -safemode leave 退出安全模式
hadoop dfsadmin -safemode enter 进入安全模式


## 添加节点

  首先在新加的节点上安装hadoop，然后修改$HADOOP_HOME/conf/master文件，加入NameNode主机名，然后在NameNode节点上修改$HADOOP_HOME/conf/slaves文件，加入新加节点主机名，再建立到新加节点无密码的SSH连接。

## 运行启动命令：

start-all.sh

然后可以通过http://(Master node的主机名):50070查看新添加的DataNode


##负载均衡

start-balancer.sh，可以使DataNode节点上选择策略重新平衡DataNode上的数据块的分布


## 工作流程

　NameNode节点作为Master服务器，有三部分功能。第一：处理来自客户端的文件访问。第二：管理文件系统的命名空间操作，如'打开'、'关闭'、'重命名'等。第三：负责数据块到数据节点之间的映射。从这个意义上说，它扮演中心服务器的角色。   
　     
　 DataNode节点作为Slave服务器，同样有三部分功能。第一：管理挂载在节点上的存储设备。第二：响应客户端的读写请求。第三：从内部 看，每个文件被分成一个或多个数据块，被存放到一组DataNode，在Namenode的统一调度下进行数据块的创建、删除和复制。     
　     
　 一个典型的部署场景是，一台GNU/Linux操作系统上运行一个Namenode实例，作为Master中 心服务器。而集群中的其它GNU/Linux操作系统分别运行一个Datanode实例，作为Slave服务器集群。
　 
## MapReduce编程模型

从概念上来讲，MapReduce将输入元素列表(Input List)转换成输出元素列表(Output List)，按照Map与Reduce规则各一次

MapReduce程序有着两个组件：一个实现了 Mapper，另一个实现了Reducer。    
第一次叫Mapping(此处结合数组的.map理解)，输入为input list,输出为output list, 并不改变输入数组，只是返回新的数组

第二次叫Reducing，MapReduce将Input List作为Reducing函数的输入参数，经过迭代处理，把这些数据汇集，返回一个输出值给Output Value。从这个意义上来说，Reducing一般用来生成”总结“数据，把大规模的数据转变成更小的总结数据。例如，"+"可以用来作一个 reducing函数，去返回输入数据列表的值的总和。

从工作流程来讲，MapReduce对应的作业Job首先把输入的数据集切分为若干独立的数据块，并由Map组件以Task的方式并行处理。处 理结果经过排序后，依次输入给Reduce组件，并且以Task的形式并行处理。MapReduce对应的输入输出数据由HDFS的DataNode存 储。MapReduce对应的Job部署在Master服务器，由Master JobTracker负责Task的调度，监控，重新执行失败的任务等等。MapReduce对应的Job部署在若干不同的Slave服务器，每个集群节 点含一个slave TaskTracker，负责执行由master指派的任务。    

Hadoop框架由Java实现的，它提供了两种主要工具。Hadoop Streaming是一种运行作业的实用工具，它允许用户创建和运行任何可执行程序(例如：Shell工具)来做为mapper和reducer。 Hadoop Pipes是一个与SWIG兼容的C++ API (没有基于JNITM技术)，它也可用于实现Map/Reduce应用程序。这样，开发人员就可以利用MapReduce框架，开发分布式应用程序，运行 在由上千个商用机器组成的大型集群上，并以一种可靠容错的方式并行处理上T级别的数据集。


##面向列开源分布式数据库Hbase

HBase是一个分布式的、面向列的开源数据库，由Apache基金会开发。HBase不同于一般的关系数据库，它是一个适合于非结构化数据存 储的数据库。它基于列的而不是基于行的模式。用户存储数据行在一个表里。一个数据行拥有一个可选择的键和任意数量的列。用户可根据键访问行，以及对于一系 列的行进行扫描和过滤。HBase一个可以横向扩张的表存储系统，能够为大规模数据提供速度极快的低等级更新。主要用于需要随机访问，实时读写大数据 (Big Data)。这正是信息系统所需要的功能。    


Hbase目标主要依靠横向扩展，通过不断增加廉价的商用服务器，来增加计算和存储能力。简单的理解，Hbase介于nosql和 RDBMS之间。Hbase仅能通过主键(row key)和主键的range来检索数据，不支持条件查询以及排序等，仅支持单行事务。Habase主要用来存储非结构化和半结构化的松散数据。针对 Hbase的不足，Hadoop的一个数据仓库工具Hive对此做出了弥补。Hive可以将结构化的数据文件映射为一张数据库表，并提供完整的SQL查询 功能，并将SQL语句转换为MapReduce任务运行。针对Hbase单行事务的限制，Hive也提供了扩展。据说，Facebook之所以选择了 Hbase，是因为他们HBase适用于处理以下两种类型的数据模式：1.一小组经常变化的临时数据;2.一组不断增加但很少访问的数据。

关于Hbase具体的知识，此处省略。