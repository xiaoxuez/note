## Kafka


[官方文档](http://kafka.apache.org/documentation/)

### 相关操作

```
	//启动server
	bin/kafka-server-start.sh config/server.properties
	
	// create topic
	bin/kafka-topics.sh --create --zookeeper localhost:2181 --replication-factor 1 --partitions 1 --topic test
	
	//list topic
	bin/kafka-topics.sh --list --zookeeper localhost:2181
	
	//produce msg to topic test
	bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test
	
	//a command line consumer
	bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning
	
	// describe topic, result includes Leader,PartitionCount, Partition...
	bin/kafka-topics.sh --describe --zookeeper localhost:2181 --topic my-replicated-topic
	
	//delete
	bin/kafka-topics.sh  --zookeeper  localhost:2181 --delete --topic tt
	
	//comsumer for key-value
	bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic WordsWithCountsTopic --from-beginning           --formatter kafka.tools.DefaultMessageFormatter           --property print.key=true           --property key.deserializer=org.apache.kafka.common.serialization.StringDeserializer           --property value.deserializer=org.apache.kafka.common.serialization.LongDeserializer
	
	//查看topic分布情况kafka-list-topic.sh
	bin/kafka-list-topic.sh --zookeeper 192.168.197.170:2181,192.168.197.171:2181 （列出所有topic的分区情况）
	bin/kafka-list-topic.sh --zookeeper 192.168.197.170:2181,192.168.197.171:2181 --topic test （查看test的分区情况）
	
	//重新分配分区kafka-reassign-partitions.sh
	//这个命令可以分区指定到想要的--broker-list上
	bin/kafka-reassign-partitions.sh --topics-to-move-json-file topics-to-move.json --broker-list "171" --zookeeper 192.168.197.170:2181,192.168.197.171:2181 --execute 
	
	//为Topic增加 partition数目kafka-add-partitions.sh
	bin/kafka-add-partitions.sh --topic test --partition 2  --zookeeper  192.168.197.170:2181,192.168.197.171:2181 （为topic test增加2个分区）
	
	//手动均衡topic, kafka-preferred-replica-election.sh
	
```

+ Kafka允许topic的分区拥有若干副本, 可以为每个topic配置副本的数量(--replication-factor)。创建副本的单位是topic的分区，每个分区都有一个Leader和零或多个followers,所有的读写操作都由leader处理，leader挂掉之后，就会在followers中选择新的leader。[详细介绍](http://www.infoq.com/cn/articles/kafka-analysis-part-2/)



### 示例分析-Kafka-stream

+ WordCountLambdaExample。单词计数，相关Api是

		  final KStreamBuilder builder = new KStreamBuilder();
			//来源, key-value-topic
		  final KStream<String, String> textLines = builder.stream(stringSerde, stringSerde, "testTopic");
		  final KTable<String, Long> wordCounts = textLines
	     	      .flatMapValues(value -> Arrays.asList(pattern.split(value.toLowerCase())))
	     	      .groupBy((key, word) -> word)
	      .count("Counts");
	      //flatMap，会把子集合的数据压缩到父集合中，就是key不变，value变化
	      //去向, key-value-topic
	       wordCounts.to(stringSerde, longSerde, "WordsWithCountsTopic");
	       final KafkaStreams streams = new KafkaStreams(builder, streamsConfiguration);
	       streams.cleanUp();
    	   streams.start();
    	   
    	   // Add shutdown hook to respond to SIGTERM and gracefully close Kafka Streams
   			Runtime.getRuntime().addShutdownHook(new Thread(streams::close));
   			
+ WikipediaFeedAvroLambdaExample, 类似单词计数。跟WordCountLambdaExample的差别如下：
	1. 来源格式不简单，value为Avro格式，所以设置相应的SpecificRecord(avro的序列化和反序列化等..)
	2. 来源使用的Api不一样, stream的参数没有指定key，value的类型，为什么可以不指定呢？..待定😳    ```final KStream<String, WikiFeed> feeds = builder.stream(WikipediaFeedAvroExample.WIKIPEDIA_FEED);```
	
	3. 流式计算中使用了filter()和map, 注意map的返回值为key-value,点一下上面flatMap， 直接返回value即可， map返回的只有子集合。    ```.map((key, value) -> new KeyValue<>(value.getUser().toString(), value))```

+ WikipediaFeedAvroExampleDriver， 作为上个例子中Producer
+ WikipediaFeedAvroExample， 跟之前的区别是在stream中的处理用的普通函数而不是lambda
+ UserRegionLambdaExample, 同样是计数，特点在于将统计出来的结果保存于KTable中， KTable可toStream再进行输出到topic中


